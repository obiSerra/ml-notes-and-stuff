* 1 - Simple Sentiment Analysis

Use a RNN (Recurring Neural Network) as they are commonly used in analysing sequences.

A RNN takes in sequence of words, X = {x1, ... xT}, one at a time and produces a hidden state (h) for each word. 
The RNN is used recurrenty, by feeding in the current word (xt) as well as the hidden state of the previous word (ht-1) to produce the next hidden state (ht)
Once we have our final hidden state (hT), we feed it through a linear layer (f) - also known as a fully connected layer - to receive our predict sentiment


One of the main concepts of TorchText is the `Field` ==> define how the data should be processed

We define `TEXT field` to define how the review should be processed and the `LABEL field` to process the sentiment
`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels.

We will use one-hot vector to represent the words in the vocabulary
Pytorch can represent the one-hot vector as an index to reduce the number of dimensions
Because the whole vocabulary would generate a vector too big, we can
+ take the top n words (that appears more time) - we will actually use this approach-
+ or drop those that appear less than


<unk> special keyword to replace the words cut from the dictionary
<pad> special keyord used to have all the batch with the same length


`BucketIterator` --> special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.
we will iterate over these iterators in the training/evaluation loop


Building the model we need to define the layers

The `embedding layer` is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector; it is a fully connected layer
The `RNN layer` is our RNN which takes our dense vector and the previous hidden state to calculate the next hidden state
The `linear layer` takes the final hidden state and feeds it through a fully connected layer transforming it to the correct output dimension 

each batch x is a tensor of size [sentence length, batch size]. That is a batch of sentences, each having each word converted into a one-hot vector
the inout batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. 
`embedded` is a tensor of size [sentence length, batch size, embedded dim]
`embedded` is then feed into the RNN
the RNN returns 2 tensors, `output` of size [sentence length, batch size, hidden dim] and `hidden` of size [1, batch size, hidden dim]
`output` is the concatenation of the hidden state for every time step
`hidden` is simply the final hidden state


* 2 - Updated Sentiment Analysis 

Different RNN Architecture called Long Short-Term Memory (LSTM)
Standard RNN suffers from the vanishing gradient problem; LSTM has an extra recurrent state called a `cell` (a sort of memory)
LSTM is a function of xt, ht and ct instead of just xt and ht

Bidirectional RNN: having a RNN to process the words from first to last (forward RNN)and a second one to process the words from last to first (backward RNN)
